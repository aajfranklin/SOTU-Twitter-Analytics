{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Appendix 1 - Data Acquisition</h2>\n",
    "\n",
    "This program makes calls to the Twitter standard search API through the module Twython [https://twython.readthedocs.io/en/latest/]. It was used to scrape over 300,000 unique tweets made during the 2018 State of the Union address, including some useful metadata.\n",
    "\n",
    "As the Twitter standard search API can only be used to search for tweets made within the last 7-10 days, the program is not currently functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules, including Twython module to handle interface with Twitter standard search API\n",
    "from twython import Twython\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Load in file containing twitter API credentials\n",
    "with open(\"twitter_credentials.json\", \"r\") as file:\n",
    "    creds = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create twython object to handle queries\n",
    "python_tweets = Twython(creds[\"CONSUMER_KEY\"], creds[\"CONSUMER_SECRET\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Total tweets to fetch\n",
    "tweets_to_fetch_count = 300000\n",
    "\n",
    "# Controls when to pause the program to ensure Twitter standard search API call limit is not breached\n",
    "request_reset_mark = 10000\n",
    "\n",
    "# Dictionary to populate with tweet data, to be turned into a dataframe later\n",
    "data = {\"user\": [],\n",
    "        \"date\": [],\n",
    "        \"text\": [],\n",
    "        \"favorite_count\": [],\n",
    "        \"id_str\": [],\n",
    "        \"location\": [],\n",
    "        \"retweet_count\": [],\n",
    "        \"followers_count\": [],\n",
    "        \"statuses_count\": [],\n",
    "        \"verified\": [],\n",
    "        \"description\": [],\n",
    "        \"coordinates\": [],\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function determines the fields of scraped tweets that will be appended to our data\n",
    "def run_query():\n",
    "\n",
    "    if (\"RT @\" not in status[\"text\"]):  # Ignore retweets, so only unique tweets are included\n",
    "        data[\"user\"].append(status[\"user\"][\"screen_name\"])\n",
    "        data[\"date\"].append(status[\"created_at\"])\n",
    "        data[\"text\"].append(status[\"text\"])\n",
    "        data[\"favorite_count\"].append(status[\"favorite_count\"])\n",
    "        data[\"id_str\"].append(status[\"id_str\"])\n",
    "        data[\"location\"].append(status[\"user\"][\"location\"])\n",
    "        data[\"retweet_count\"].append(status[\"retweet_count\"])\n",
    "        data[\"followers_count\"].append(status[\"user\"][\"followers_count\"])\n",
    "        data[\"statuses_count\"].append(status[\"user\"][\"statuses_count\"])\n",
    "        data[\"verified\"].append(status[\"user\"][\"verified\"])\n",
    "        data[\"description\"].append(status[\"user\"][\"description\"])\n",
    "\n",
    "for i in range (0, 30000):\n",
    "    \n",
    "    # Stop program following the call that exceeds 300,000 tweets scraped\n",
    "    if(len(data[\"id_str\"]) > tweets_to_fetch_count):  \n",
    "        break\n",
    "    \n",
    "    # Pause the program for 15 minutes for every 10,000 tweets scraped, to avoid exceeding API call limit\n",
    "    if(len(data[\"id_str\"]) > request_reset_mark):\n",
    "        time.sleep(60*15)\n",
    "        request_reset_mark += 10000\n",
    "    \n",
    "    # Set first query\n",
    "    if(i == 0):\n",
    "        query = {\"q\": \"#sotu\",\n",
    "                 \"lang\": \"en\",\n",
    "                 \"until\": \"2018-02-01\",\n",
    "                 \n",
    "                 # The maximum id of the first tweet to scrape. The tweet id is a unique identifier for all tweets\n",
    "                 # This max_id value represents a point shortly after the end of the SOTU address \n",
    "                 # It was attained through trial and error \n",
    "                 \"max_id\": 958543500000000000\n",
    "                }\n",
    "        \n",
    "        # Call a search with the twython object, appending relevant information to data as specified in run_query\n",
    "        for status in python_tweets.search(**query, count = 100)[\"statuses\"]:\n",
    "            run_query()\n",
    "    \n",
    "    # Set remaining queries\n",
    "    else:\n",
    "        query = {\"q\": \"#sotu\",\n",
    "                 \"lang\": \"en\",\n",
    "                 \"until\": \"2018-02-01\",\n",
    "                 \"max_id\": max_id\n",
    "                }\n",
    "        \n",
    "        # Call a search with the twython object, appending relevant information to data as specified in run_query\n",
    "        for status in python_tweets.search(**query, count = 100)[\"statuses\"]:\n",
    "            run_query()\n",
    "    \n",
    "    # Set the new max_id to the lowest id attained so far\n",
    "    # This ensures the program moves backwards through tweets from the specified starting id\n",
    "    max_id = min(data[\"id_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert data into a DataFrame\n",
    "frame = pd.DataFrame(data)\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write data out to an excel file\n",
    "writer = pd.ExcelWriter('output.xlsx')\n",
    "frame.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
