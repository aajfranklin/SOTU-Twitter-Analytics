{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Appendix 3 - Cleaning Text Fields</h2>\n",
    "\n",
    "Our data includes four text fields that require pre-processing prior to analysis: User, Location, Description, and Text. The steps set out in plain text below are described in greater detail in the Pre-Processing section of our study.\n",
    "\n",
    "**Steps for all fields**\n",
    "<ul>\n",
    "<li>Capitalise text to simplify future token matching</li>\n",
    "</ul>\n",
    "\n",
    "**Steps for Location and User**\n",
    "<ul>\n",
    "<li>Remove all punctuation</li>\n",
    "</ul>\n",
    "\n",
    "**Steps for Description and Text**\n",
    "<ul>\n",
    "<li>Remove URLs in truncated tweets</li>\n",
    "<li>Remove all punctuation except \"#\" and \"@\", used to identify hashtags and user mentions</li>\n",
    "<li>Identify hashtags and mentions, stripping them from the original fields and preserving them in new columns</li>\n",
    "</ul>\n",
    "\n",
    "**Steps for Text only**\n",
    "<ul>\n",
    "<li>Split camelcase hashtags into composite words for use in sentiment analysis</li>\n",
    "<li>Remove emoji, preserving them in a new column</li>\n",
    "</ul>\n",
    "\n",
    "The unicode value ranges used to identify emoji were taken from https://apps.timwhitlock.info/emoji/tables/unicode. The system is clearly imperfect as a number of emoji are not successfully identified. As such, they were not used for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_data = pd.read_excel(\"sotu_final_corpus.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create empty columns to populate later\n",
    "tweet_data[\"description_hashtags\"] = \"\"\n",
    "tweet_data[\"description_mentions\"] = \"\"\n",
    "tweet_data[\"text_emoji\"] = \"\"\n",
    "tweet_data[\"text_hashtags\"] = \"\"\n",
    "tweet_data[\"text_hashtags_split\"] = \"\"\n",
    "tweet_data[\"text_mentions\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write regex pattern to remove all punctuation\n",
    "remove = string.punctuation\n",
    "remove = remove + \"“”‘’\"\n",
    "punct_pattern = r\"[{}]\".format(remove)\n",
    "\n",
    "# Write second regex pattern to remove all punctuation except '#' and '@'\n",
    "remove = remove.replace(\"#\", \"\")\n",
    "remove = remove.replace(\"@\", \"\")\n",
    "special_punct_pattern = r\"[{}]\".format(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(tweet_data.index)):\n",
    "    \n",
    "    # Capitalise and remove puncutation from user field\n",
    "    tweet_data.at[i,\"user\"] = re.sub(punct_pattern, \" \", tweet_data.at[i,\"user\"]).upper()\n",
    "    \n",
    "    # Capitalise and remove punctuation from location field, avoiding empty fields\n",
    "    location = tweet_data.at[i,\"location\"]\n",
    "    if type(location) == str:\n",
    "        tweet_data.at[i,\"location\"] = re.sub(punct_pattern, \" \", location).upper()    \n",
    "    \n",
    "    # Clean description field\n",
    "    description = tweet_data.at[i,\"description\"]\n",
    "    if type(description) == str:\n",
    "        description = re.sub(r\"http\\S+\", \"\", description)  # Remove URLs\n",
    "        description = re.sub(special_punct_pattern, \" \", description)  # Remove punctuation\n",
    "        \n",
    "        # Strip hashtags and mentions into separate column\n",
    "        tweet_data.at[i,\"description_hashtags\"] = str.join(\" \",[word.strip(\"#\") for word in description.split() if word.startswith(\"#\")]).upper()\n",
    "        tweet_data.at[i,\"description_mentions\"] = str.join(\" \",[word.strip(\"@\") for word in description.split() if word.startswith(\"@\")]).upper()\n",
    "        tweet_data.at[i,\"description\"] = str.join(\" \",[word for word in description.split() if (not word.startswith(\"#\")) and (not word.startswith(\"@\"))]).upper()\n",
    "        \n",
    "    # Clean text field\n",
    "    text = tweet_data.at[i,\"text\"]\n",
    "    if type(text) == str:\n",
    "        text = re.sub(r\"http\\S+\", \"\", text) # Remove URLs\n",
    "        text = re.sub(special_punct_pattern, \" \", text) # Remove punctuation\n",
    "        \n",
    "        # Strip emoji into separate column\n",
    "        tweet_data.at[i, \"text_emoji\"] = str.join(\" \", [word for word in text.split() if (('\\U0001f601' <= word <= '\\U0001f64f') or ('\\U00002702' <= word <= '\\U000027b0') or ('\\U0001f680' <= word <= '\\U0001f6c5') or ('\\U0001f170' <= word <= '\\U0001f251') or ('\\U0001f300' <= word <= '\\U0001f5ff') or word == '\\U0000236a')])\n",
    "        text = str.join(\" \", [word for word in text.split() if word not in (tweet_data.at[i, \"text_emoji\"])])\n",
    "        \n",
    "        # Strip hashtags and mentions into separate columns\n",
    "        tweet_data.at[i,\"text_hashtags\"] = str.join(\" \",[word.strip(\"#\") for word in text.split() if word.startswith(\"#\")])\n",
    "        tweet_data.at[i,\"text_mentions\"] = str.join(\" \",[word.strip(\"@\") for word in text.split() if word.startswith(\"@\")]).upper()\n",
    "        tweet_data.at[i,\"text\"] = str.join(\" \",[word for word in text.split() if (not word.startswith(\"#\")) and (not word.startswith(\"@\"))]).upper()\n",
    "        \n",
    "        # Split camel case hashtags\n",
    "        hashtags = tweet_data.at[i,\"text_hashtags\"]\n",
    "        tweet_data.at[i,\"text_hashtags_split\"] = re.sub(r'((?<=[a-z])[A-Z]|(?<!\\A)[A-Z](?=[a-z]))', r' \\1', hashtags).upper()\n",
    "        tweet_data.at[i,\"text_hashtags\"] = tweet_data.at[i,\"text_hashtags\"].upper()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save data set\n",
    "writer = pd.ExcelWriter('sotu_processed.xlsx')\n",
    "tweet_data.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
