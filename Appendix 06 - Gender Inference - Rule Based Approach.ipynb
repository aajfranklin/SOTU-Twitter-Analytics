{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 6 Gender Inference - Rule Based Approach\n",
    "\n",
    "### Part 1: Username\n",
    "\n",
    "Using the human-classified sample of 1000 random rows from our dataset of tweets, this program crosschecks all substrings of the User field against a list of known male and female first names, and makes a gender classification where it finds a match.\n",
    "\n",
    "#### Names dataset & adjustments made to it\n",
    "We took a dataset (https://github.com/organisciak/names/blob/master/data/us-likelihood-of-gender-by-name-in-2014.csv) of the top 1000 female names and top 1000 male names in the US in 2014. For any name that featured in both the male and female lists, we cross-checked its gender against the probabilty list and assigned it to the gender with the highest statistical probability for that name. In addition to this, based on some early experiments, we added some words which were not names but clear signifiers of gender commonly used in usernames, eg \"girl\", \"chick\", \"momma\".\n",
    "\n",
    "#### Other details\n",
    "We matched names in case insensitive terms, as many usernames do not use standard name capitalisation.\n",
    "\n",
    "Further, we removed names of 3 characters or less, as these led to a high proportion of inaccurate matches (eg username AzanianSea contained a match for the female name \"Nia\", username XMASTIMEblog contained a match for the male name \"Tim\" when in reality neither username should be associated with a particular gender)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>description</th>\n",
       "      <th>user</th>\n",
       "      <th>gender_username_human</th>\n",
       "      <th>gender_description_human</th>\n",
       "      <th>gender_human_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>123</td>\n",
       "      <td>I didn't mean to call you an angry mob. Mama a...</td>\n",
       "      <td>LM_Shepard</td>\n",
       "      <td>U</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>377</td>\n",
       "      <td>Economist, opinion columnist, libertarian, Geo...</td>\n",
       "      <td>DorfmanJeffrey</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1664</td>\n",
       "      <td>#mediadopedealer A Willy Wonka creation ðŸ‘€ and ...</td>\n",
       "      <td>Ashlee_Ray</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>2087</td>\n",
       "      <td>Author of the book Meridian Hill Park. License...</td>\n",
       "      <td>feejaysee</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>2202</td>\n",
       "      <td>I bleed Red, White and Blue. God bless Texas. ...</td>\n",
       "      <td>lapadooza</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>2260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DanKronstadt</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>2310</td>\n",
       "      <td>Wife, Mom, Teacher. Just trying to make sense ...</td>\n",
       "      <td>JLaufe</td>\n",
       "      <td>U</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>2622</td>\n",
       "      <td>bon vivant/raconteur/troubadour \\nopinionated ...</td>\n",
       "      <td>MichaelSalamone</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3002</td>\n",
       "      <td>Just an introvert forever trying to break out ...</td>\n",
       "      <td>MissElsa86</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>3346</td>\n",
       "      <td>I enjoy being around my Twitter family on here...</td>\n",
       "      <td>tom_lewisville</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3408</td>\n",
       "      <td>Dagny's mom. Ann Coulter reader. Used Books. L...</td>\n",
       "      <td>AudreyAnnW</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>3809</td>\n",
       "      <td>It was probably Chris.</td>\n",
       "      <td>itwaschris</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>4033</td>\n",
       "      <td>Writing about life, parenthood, cooking and mo...</td>\n",
       "      <td>jamieloujam</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4213</td>\n",
       "      <td>Mom of 2 kids, lives on a farm, raising chicke...</td>\n",
       "      <td>Angela_McCoy003</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4567</td>\n",
       "      <td>Dad to #ThePeoplesChamp and #ThePintSizedPugil...</td>\n",
       "      <td>joelwaldmanNEWS</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>4570</td>\n",
       "      <td>First of her name. The Easily Burnt. Khaleesi ...</td>\n",
       "      <td>grremlinsmama</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>4650</td>\n",
       "      <td>pass me the sphygmomanometer</td>\n",
       "      <td>themikaylakohls</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5056</td>\n",
       "      <td>world traveler. frenchie mom. we will no longe...</td>\n",
       "      <td>haley_gray11</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>5203</td>\n",
       "      <td>Owner of 20nine, graphic designer, brand strat...</td>\n",
       "      <td>Greg20nine</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>5326</td>\n",
       "      <td>26/F, PhD Candidate in Materials Engineering, ...</td>\n",
       "      <td>Aliseyun</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>5378</td>\n",
       "      <td>Senior Fellow @CEIdotorg. Member of no politic...</td>\n",
       "      <td>michelleminton</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>5492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cchesser60</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>6233</td>\n",
       "      <td>The latest political news from HuffPost's poli...</td>\n",
       "      <td>HuffPostPol</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>6417</td>\n",
       "      <td>Eater, drinker, general annoyance.</td>\n",
       "      <td>markymark_69</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>6418</td>\n",
       "      <td>It's my party &amp; I post what I want to, but I t...</td>\n",
       "      <td>GraciousKY</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>6465</td>\n",
       "      <td>14 yr old filly.</td>\n",
       "      <td>SassyHorsey</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>6495</td>\n",
       "      <td>Fine Artist &amp; Designer in Brooklyn NY. Designs...</td>\n",
       "      <td>ANoelleJay</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>6766</td>\n",
       "      <td>Opinions on this page are my own.</td>\n",
       "      <td>gillvernment</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>6967</td>\n",
       "      <td>Livin' the dream.</td>\n",
       "      <td>AustinHinderer5</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>7860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sustagurlsgurl</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>289167</td>\n",
       "      <td>With God\\nPolyvore Fashion Blogger \\nPersonal ...</td>\n",
       "      <td>Lovely_Ressey</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>289574</td>\n",
       "      <td>Employment and Industrial Lawyer. Fed up with ...</td>\n",
       "      <td>kelsytomo</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>289650</td>\n",
       "      <td>Regional Field Director for PAGOP, Master's St...</td>\n",
       "      <td>MrGivens_91</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>291276</td>\n",
       "      <td>Entertainment; soley interested in my own ente...</td>\n",
       "      <td>metrofla</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>291334</td>\n",
       "      <td>I told you my Tweets would take you places. I ...</td>\n",
       "      <td>HissCFitt</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>291467</td>\n",
       "      <td>Public Citizen is a national, nonprofit advoca...</td>\n",
       "      <td>Public_Citizen</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>291584</td>\n",
       "      <td>Dad, Husband, Engineer, Bills Fan, Star Wars G...</td>\n",
       "      <td>skamz23</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>291660</td>\n",
       "      <td>I Tweet Therefore I Am. English PhD Student.</td>\n",
       "      <td>Ellojoe</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>292233</td>\n",
       "      <td>The Suffolk Journal is the award-winning under...</td>\n",
       "      <td>SuffolkJournal</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>292664</td>\n",
       "      <td>Baldwin â€¢ Certified Hat Expert â€¢ Groucho Marxi...</td>\n",
       "      <td>TheHat2</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>293100</td>\n",
       "      <td>Founder, @fourth_nine | @UConn alum | Back in ...</td>\n",
       "      <td>DylanADeSimone</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>293312</td>\n",
       "      <td>Living proof that everything happens for a rea...</td>\n",
       "      <td>Mswendy2004</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>293536</td>\n",
       "      <td>Co-founder of Stone Circle Games. Creator of M...</td>\n",
       "      <td>atfennell</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>293826</td>\n",
       "      <td>In short; a Kurd</td>\n",
       "      <td>blcm_brlk</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>293930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DigitalAges</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>293947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RonLammardo</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>295015</td>\n",
       "      <td>I know that G-d loves America because He gave ...</td>\n",
       "      <td>IHeartSully</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>295036</td>\n",
       "      <td>Î¤ÎµÎ»ÎµÎ¹Ï‰Î¿Î¹Ï‚ â€¢ The perfect stranger â€¢ Hustlers Sp...</td>\n",
       "      <td>Kim0Br0wn</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>295947</td>\n",
       "      <td>( img: via @interior GG Natl Pk) - instagram: ...</td>\n",
       "      <td>jackhutton</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>296803</td>\n",
       "      <td>Content and copywriter available to add some p...</td>\n",
       "      <td>leesu44</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>296977</td>\n",
       "      <td>i embrace a both/and | dedicated to a more lov...</td>\n",
       "      <td>itsAlexCL</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>297265</td>\n",
       "      <td>Poet, Teacher, and Guerrilla Tweetist. Former ...</td>\n",
       "      <td>RightPoetRight</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>298053</td>\n",
       "      <td>#TheWorldNeedsLove\\n#BlackUnity\\n#ProudVeteran...</td>\n",
       "      <td>LyfeLessons1978</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>298445</td>\n",
       "      <td>I'm a real, live Anglican priest in the United...</td>\n",
       "      <td>pastormac1usa</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>298790</td>\n",
       "      <td>Froggug sederty bungo-bungo! Proud to be atten...</td>\n",
       "      <td>Thunder_Hedgie</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>299200</td>\n",
       "      <td>Bringing you news, weather and sports in the A...</td>\n",
       "      <td>CBS6Albany</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>299566</td>\n",
       "      <td>Exercise your right to express yourself. Speak...</td>\n",
       "      <td>authprotester</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>299602</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tupacwasagemini</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>299882</td>\n",
       "      <td>Dad~LunchLady~Volunteer Currently grinding for...</td>\n",
       "      <td>KevinBrashear</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>299969</td>\n",
       "      <td>US Navy MM2(NUKE/SW) 2002-08, Uncle, Owns cat,...</td>\n",
       "      <td>mjwatts1983</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                        description  \\\n",
       "193     123  I didn't mean to call you an angry mob. Mama a...   \n",
       "270     377  Economist, opinion columnist, libertarian, Geo...   \n",
       "104    1664  #mediadopedealer A Willy Wonka creation ðŸ‘€ and ...   \n",
       "366    2087  Author of the book Meridian Hill Park. License...   \n",
       "906    2202  I bleed Red, White and Blue. God bless Texas. ...   \n",
       "397    2260                                                NaN   \n",
       "482    2310  Wife, Mom, Teacher. Just trying to make sense ...   \n",
       "332    2622  bon vivant/raconteur/troubadour \\nopinionated ...   \n",
       "20     3002  Just an introvert forever trying to break out ...   \n",
       "359    3346  I enjoy being around my Twitter family on here...   \n",
       "23     3408  Dagny's mom. Ann Coulter reader. Used Books. L...   \n",
       "372    3809                             It was probably Chris.   \n",
       "225    4033  Writing about life, parenthood, cooking and mo...   \n",
       "25     4213  Mom of 2 kids, lives on a farm, raising chicke...   \n",
       "45     4567  Dad to #ThePeoplesChamp and #ThePintSizedPugil...   \n",
       "469    4570  First of her name. The Easily Burnt. Khaleesi ...   \n",
       "617    4650                       pass me the sphygmomanometer   \n",
       "26     5056  world traveler. frenchie mom. we will no longe...   \n",
       "382    5203  Owner of 20nine, graphic designer, brand strat...   \n",
       "155    5326  26/F, PhD Candidate in Materials Engineering, ...   \n",
       "165    5378  Senior Fellow @CEIdotorg. Member of no politic...   \n",
       "642    5492                                                NaN   \n",
       "797    6233  The latest political news from HuffPost's poli...   \n",
       "404    6417                 Eater, drinker, general annoyance.   \n",
       "535    6418  It's my party & I post what I want to, but I t...   \n",
       "988    6465                                   14 yr old filly.   \n",
       "97     6495  Fine Artist & Designer in Brooklyn NY. Designs...   \n",
       "765    6766                  Opinions on this page are my own.   \n",
       "243    6967                                  Livin' the dream.   \n",
       "186    7860                                                NaN   \n",
       "..      ...                                                ...   \n",
       "529  289167  With God\\nPolyvore Fashion Blogger \\nPersonal ...   \n",
       "81   289574  Employment and Industrial Lawyer. Fed up with ...   \n",
       "430  289650  Regional Field Director for PAGOP, Master's St...   \n",
       "965  291276  Entertainment; soley interested in my own ente...   \n",
       "937  291334  I told you my Tweets would take you places. I ...   \n",
       "750  291467  Public Citizen is a national, nonprofit advoca...   \n",
       "523  291584  Dad, Husband, Engineer, Bills Fan, Star Wars G...   \n",
       "298  291660       I Tweet Therefore I Am. English PhD Student.   \n",
       "898  292233  The Suffolk Journal is the award-winning under...   \n",
       "891  292664  Baldwin â€¢ Certified Hat Expert â€¢ Groucho Marxi...   \n",
       "562  293100  Founder, @fourth_nine | @UConn alum | Back in ...   \n",
       "50   293312  Living proof that everything happens for a rea...   \n",
       "608  293536  Co-founder of Stone Circle Games. Creator of M...   \n",
       "823  293826                                   In short; a Kurd   \n",
       "557  293930                                                NaN   \n",
       "251  293947                                                NaN   \n",
       "589  295015  I know that G-d loves America because He gave ...   \n",
       "46   295036  Î¤ÎµÎ»ÎµÎ¹Ï‰Î¿Î¹Ï‚ â€¢ The perfect stranger â€¢ Hustlers Sp...   \n",
       "260  295947  ( img: via @interior GG Natl Pk) - instagram: ...   \n",
       "392  296803  Content and copywriter available to add some p...   \n",
       "698  296977  i embrace a both/and | dedicated to a more lov...   \n",
       "758  297265  Poet, Teacher, and Guerrilla Tweetist. Former ...   \n",
       "741  298053  #TheWorldNeedsLove\\n#BlackUnity\\n#ProudVeteran...   \n",
       "783  298445  I'm a real, live Anglican priest in the United...   \n",
       "555  298790  Froggug sederty bungo-bungo! Proud to be atten...   \n",
       "572  299200  Bringing you news, weather and sports in the A...   \n",
       "526  299566  Exercise your right to express yourself. Speak...   \n",
       "588  299602                                                NaN   \n",
       "205  299882  Dad~LunchLady~Volunteer Currently grinding for...   \n",
       "506  299969  US Navy MM2(NUKE/SW) 2002-08, Uncle, Owns cat,...   \n",
       "\n",
       "                user gender_username_human gender_description_human  \\\n",
       "193       LM_Shepard                     U                        F   \n",
       "270   DorfmanJeffrey                     M                        U   \n",
       "104       Ashlee_Ray                     F                        U   \n",
       "366        feejaysee                     M                        U   \n",
       "906        lapadooza                     U                        U   \n",
       "397     DanKronstadt                     M                        U   \n",
       "482           JLaufe                     U                        F   \n",
       "332  MichaelSalamone                     M                        U   \n",
       "20        MissElsa86                     F                        F   \n",
       "359   tom_lewisville                     M                        U   \n",
       "23        AudreyAnnW                     F                        F   \n",
       "372       itwaschris                     M                        U   \n",
       "225      jamieloujam                     F                        U   \n",
       "25   Angela_McCoy003                     F                        F   \n",
       "45   joelwaldmanNEWS                     M                        M   \n",
       "469    grremlinsmama                     F                        F   \n",
       "617  themikaylakohls                     F                        U   \n",
       "26      haley_gray11                     F                        F   \n",
       "382       Greg20nine                     M                        U   \n",
       "155         Aliseyun                     F                        U   \n",
       "165   michelleminton                     F                        U   \n",
       "642       cchesser60                     U                        U   \n",
       "797      HuffPostPol                     U                        U   \n",
       "404     markymark_69                     M                        U   \n",
       "535       GraciousKY                     U                        U   \n",
       "988      SassyHorsey                     U                        U   \n",
       "97        ANoelleJay                     F                        U   \n",
       "765     gillvernment                     U                        U   \n",
       "243  AustinHinderer5                     M                        U   \n",
       "186   sustagurlsgurl                     F                        U   \n",
       "..               ...                   ...                      ...   \n",
       "529    Lovely_Ressey                     F                        U   \n",
       "81         kelsytomo                     U                        U   \n",
       "430      MrGivens_91                     M                        U   \n",
       "965         metrofla                     U                        U   \n",
       "937        HissCFitt                     U                        U   \n",
       "750   Public_Citizen                     U                        U   \n",
       "523          skamz23                     U                        M   \n",
       "298          Ellojoe                     M                        U   \n",
       "898   SuffolkJournal                     U                        U   \n",
       "891          TheHat2                     U                        U   \n",
       "562   DylanADeSimone                     F                        U   \n",
       "50       Mswendy2004                     F                        U   \n",
       "608        atfennell                     U                        U   \n",
       "823        blcm_brlk                     U                        U   \n",
       "557      DigitalAges                     U                        U   \n",
       "251      RonLammardo                     M                        U   \n",
       "589      IHeartSully                     U                        U   \n",
       "46         Kim0Br0wn                     F                        M   \n",
       "260       jackhutton                     M                        U   \n",
       "392          leesu44                     M                        U   \n",
       "698        itsAlexCL                     M                        U   \n",
       "758   RightPoetRight                     U                        U   \n",
       "741  LyfeLessons1978                     U                        U   \n",
       "783    pastormac1usa                     U                        U   \n",
       "555   Thunder_Hedgie                     U                        U   \n",
       "572       CBS6Albany                     U                        U   \n",
       "526    authprotester                     U                        U   \n",
       "588  Tupacwasagemini                     U                        U   \n",
       "205    KevinBrashear                     M                        M   \n",
       "506      mjwatts1983                     U                        M   \n",
       "\n",
       "    gender_human_final  \n",
       "193                  F  \n",
       "270                  M  \n",
       "104                  F  \n",
       "366                  M  \n",
       "906                  U  \n",
       "397                  M  \n",
       "482                  F  \n",
       "332                  M  \n",
       "20                   F  \n",
       "359                  M  \n",
       "23                   F  \n",
       "372                  M  \n",
       "225                  F  \n",
       "25                   F  \n",
       "45                   M  \n",
       "469                  F  \n",
       "617                  F  \n",
       "26                   F  \n",
       "382                  M  \n",
       "155                  F  \n",
       "165                  F  \n",
       "642                  U  \n",
       "797                  U  \n",
       "404                  M  \n",
       "535                  U  \n",
       "988                  U  \n",
       "97                   F  \n",
       "765                  U  \n",
       "243                  M  \n",
       "186                  F  \n",
       "..                 ...  \n",
       "529                  F  \n",
       "81                   U  \n",
       "430                  M  \n",
       "965                  U  \n",
       "937                  U  \n",
       "750                  U  \n",
       "523                  M  \n",
       "298                  M  \n",
       "898                  U  \n",
       "891                  U  \n",
       "562                  F  \n",
       "50                   F  \n",
       "608                  U  \n",
       "823                  U  \n",
       "557                  U  \n",
       "251                  M  \n",
       "589                  U  \n",
       "46                   F  \n",
       "260                  M  \n",
       "392                  M  \n",
       "698                  M  \n",
       "758                  U  \n",
       "741                  U  \n",
       "783                  U  \n",
       "555                  U  \n",
       "572                  U  \n",
       "526                  U  \n",
       "588                  U  \n",
       "205                  M  \n",
       "506                  M  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the human-classified 1000-row sample\n",
    "tweets_sample1000_withgender = pd.read_excel('tweets_sample1000.xlsx')\n",
    "tweets_sample1000_withgender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2013, 4)\n",
      "(1944, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>prob</th>\n",
       "      <th>namelength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maximiliano</td>\n",
       "      <td>M</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Christopher</td>\n",
       "      <td>M</td>\n",
       "      <td>0.996485</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bernadette</td>\n",
       "      <td>F</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cristopher</td>\n",
       "      <td>M</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alessandra</td>\n",
       "      <td>F</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Antoinette</td>\n",
       "      <td>F</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alexandria</td>\n",
       "      <td>F</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Maximilian</td>\n",
       "      <td>M</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Marguerite</td>\n",
       "      <td>F</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kristopher</td>\n",
       "      <td>M</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jacqueline</td>\n",
       "      <td>F</td>\n",
       "      <td>0.999328</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Jacquelyn</td>\n",
       "      <td>F</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mackenzie</td>\n",
       "      <td>F</td>\n",
       "      <td>0.951173</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Genevieve</td>\n",
       "      <td>F</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Dominique</td>\n",
       "      <td>F</td>\n",
       "      <td>0.706665</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Stephanie</td>\n",
       "      <td>F</td>\n",
       "      <td>0.998584</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Gabrielle</td>\n",
       "      <td>F</td>\n",
       "      <td>0.999559</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>F</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Alejandro</td>\n",
       "      <td>M</td>\n",
       "      <td>0.995838</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Rigoberto</td>\n",
       "      <td>M</td>\n",
       "      <td>0.999244</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Demetrius</td>\n",
       "      <td>M</td>\n",
       "      <td>0.980290</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Nicolette</td>\n",
       "      <td>F</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Francisco</td>\n",
       "      <td>M</td>\n",
       "      <td>0.996604</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Alexandra</td>\n",
       "      <td>F</td>\n",
       "      <td>0.998976</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Guillermo</td>\n",
       "      <td>M</td>\n",
       "      <td>0.999553</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ernestine</td>\n",
       "      <td>F</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Elisabeth</td>\n",
       "      <td>F</td>\n",
       "      <td>0.999779</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Kimberlee</td>\n",
       "      <td>F</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sylvester</td>\n",
       "      <td>M</td>\n",
       "      <td>0.999782</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sebastian</td>\n",
       "      <td>M</td>\n",
       "      <td>0.999536</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>Eden</td>\n",
       "      <td>F</td>\n",
       "      <td>0.903839</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>Rene</td>\n",
       "      <td>M</td>\n",
       "      <td>0.705724</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>0.997826</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>Dave</td>\n",
       "      <td>M</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>Kirk</td>\n",
       "      <td>M</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>Cara</td>\n",
       "      <td>F</td>\n",
       "      <td>0.999861</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>Abel</td>\n",
       "      <td>M</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>Drew</td>\n",
       "      <td>M</td>\n",
       "      <td>0.943274</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>Greg</td>\n",
       "      <td>M</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>Rick</td>\n",
       "      <td>M</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>Lyle</td>\n",
       "      <td>M</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>Gene</td>\n",
       "      <td>M</td>\n",
       "      <td>0.984815</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>Wade</td>\n",
       "      <td>M</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>Kurt</td>\n",
       "      <td>M</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>Jody</td>\n",
       "      <td>F</td>\n",
       "      <td>0.648924</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>Cora</td>\n",
       "      <td>F</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>Karl</td>\n",
       "      <td>M</td>\n",
       "      <td>0.999390</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>Otto</td>\n",
       "      <td>M</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>Iris</td>\n",
       "      <td>F</td>\n",
       "      <td>0.999957</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>Joni</td>\n",
       "      <td>F</td>\n",
       "      <td>0.999629</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>Macy</td>\n",
       "      <td>F</td>\n",
       "      <td>0.999674</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>Noel</td>\n",
       "      <td>M</td>\n",
       "      <td>0.788477</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>Lori</td>\n",
       "      <td>F</td>\n",
       "      <td>0.999744</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>Andy</td>\n",
       "      <td>M</td>\n",
       "      <td>0.998985</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>Dirk</td>\n",
       "      <td>M</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>Raul</td>\n",
       "      <td>M</td>\n",
       "      <td>0.997520</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>Lara</td>\n",
       "      <td>F</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>Vera</td>\n",
       "      <td>F</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>Zane</td>\n",
       "      <td>M</td>\n",
       "      <td>0.999737</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>Gwen</td>\n",
       "      <td>F</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1944 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name gender      prob namelength\n",
       "0     Maximiliano      M  1.000000         11\n",
       "1     Christopher      M  0.996485         11\n",
       "2      Bernadette      F  1.000000         10\n",
       "3      Cristopher      M  1.000000         10\n",
       "4      Alessandra      F  1.000000         10\n",
       "5      Antoinette      F  1.000000         10\n",
       "6      Alexandria      F  0.999682         10\n",
       "7      Maximilian      M  1.000000         10\n",
       "8      Marguerite      F  1.000000         10\n",
       "9      Kristopher      M  0.999799         10\n",
       "10     Jacqueline      F  0.999328         10\n",
       "11      Jacquelyn      F  1.000000          9\n",
       "12      Mackenzie      F  0.951173          9\n",
       "13      Genevieve      F  1.000000          9\n",
       "14      Dominique      F  0.706665          9\n",
       "15      Stephanie      F  0.998584          9\n",
       "16      Gabrielle      F  0.999559          9\n",
       "17      Charlotte      F  0.999968          9\n",
       "18      Alejandro      M  0.995838          9\n",
       "19      Rigoberto      M  0.999244          9\n",
       "20      Demetrius      M  0.980290          9\n",
       "21      Nicolette      F  1.000000          9\n",
       "22      Francisco      M  0.996604          9\n",
       "23      Alexandra      F  0.998976          9\n",
       "24      Guillermo      M  0.999553          9\n",
       "25      Ernestine      F  1.000000          9\n",
       "26      Elisabeth      F  0.999779          9\n",
       "27      Kimberlee      F  1.000000          9\n",
       "28      Sylvester      M  0.999782          9\n",
       "29      Sebastian      M  0.999536          9\n",
       "...           ...    ...       ...        ...\n",
       "1914         Eden      F  0.903839          4\n",
       "1915         Rene      M  0.705724          4\n",
       "1916         Mary      F  0.997826          4\n",
       "1917         Dave      M  1.000000          4\n",
       "1918         Kirk      M  1.000000          4\n",
       "1919         Cara      F  0.999861          4\n",
       "1920         Abel      M  1.000000          4\n",
       "1921         Drew      M  0.943274          4\n",
       "1922         Greg      M  1.000000          4\n",
       "1923         Rick      M  1.000000          4\n",
       "1924         Lyle      M  0.999734          4\n",
       "1925         Gene      M  0.984815          4\n",
       "1926         Wade      M  1.000000          4\n",
       "1927         Kurt      M  1.000000          4\n",
       "1928         Jody      F  0.648924          4\n",
       "1929         Cora      F  1.000000          4\n",
       "1930         Karl      M  0.999390          4\n",
       "1931         Otto      M  1.000000          4\n",
       "1932         Iris      F  0.999957          4\n",
       "1933         Joni      F  0.999629          4\n",
       "1934         Macy      F  0.999674          4\n",
       "1935         Noel      M  0.788477          4\n",
       "1936         Lori      F  0.999744          4\n",
       "1937         Andy      M  0.998985          4\n",
       "1938         Dirk      M  1.000000          4\n",
       "1939         Raul      M  0.997520          4\n",
       "1940         Lara      F  1.000000          4\n",
       "1941         Vera      F  1.000000          4\n",
       "1942         Zane      M  0.999737          4\n",
       "1943         Gwen      F  1.000000          4\n",
       "\n",
       "[1944 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset of names and their genders\n",
    "namesfinal = pd.read_excel(\"top2000names2014final.xlsx\")\n",
    "\n",
    "# Sort names dataframe so longest names come first (so that Erica will then match against Erica and not Eric)\n",
    "namesfinal[\"namelength\"] = \"\"\n",
    "for i in range(len(namesfinal)):\n",
    "    namesfinal.at[i, \"namelength\"] = len(namesfinal.at[i, \"name\"])\n",
    "namesfinal = namesfinal.sort_values(by=['namelength'], ascending=False)\n",
    "namesfinal = namesfinal.reset_index(drop=True)\n",
    "print(namesfinal.shape)\n",
    "\n",
    "# Remove any names 3 characters or less\n",
    "namesfinal.drop(namesfinal[namesfinal.namelength < 4].index, inplace=True)\n",
    "print(namesfinal.shape)\n",
    "namesfinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a column with username converted to lowercase, on both datasets, to enable case insensitive matching\n",
    "tweets_sample1000_withgender[\"user_lower\"] = tweets_sample1000_withgender[\"user\"].str.lower()\n",
    "namesfinal[\"name_lower\"] = namesfinal[\"name\"].str.lower()\n",
    "\n",
    "# Create 2 columns on our dataset of tweets for the algorithm to make its gender classifications in\n",
    "tweets_sample1000_withgender[\"gender_username_algo\"] = \"\"\n",
    "tweets_sample1000_withgender[\"namematched_algo\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "300\n",
      "600\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "# External loop iterates through our dataset of tweets\n",
    "for i in range(len(tweets_sample1000_withgender)):\n",
    "    # Internal loop iterates through the list of names\n",
    "    for j in range (len(namesfinal)):\n",
    "        # This step ensures that shorter names do not override longer names when matching\n",
    "        if tweets_sample1000_withgender.at[i, \"gender_username_algo\"]==\"\":\n",
    "            # If a name is found within the user field, the corresponding gender is assigned and (for evaluation purposes), the name matched on is recorded\n",
    "            if (namesfinal.at[j, \"name_lower\"] in tweets_sample1000_withgender.at[i, \"user_lower\"]):\n",
    "                tweets_sample1000_withgender.at[i, \"gender_username_algo\"] = namesfinal.at[j, \"gender\"]\n",
    "                tweets_sample1000_withgender.at[i, \"namematched_algo\"] = namesfinal.at[j, \"name\"]\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "    # Just to get a sense of progress while processing\n",
    "    if i%300==0:\n",
    "        (print(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the algorithm's username-based classifications\n",
    "\n",
    "We analysed the algorithm's username based classifications in terms of quantity and accuracy against the human's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Count of classifications\n",
      "gender_username_algo                          \n",
      "                                           562\n",
      "F                                          179\n",
      "M                                          259 \n",
      "\n",
      "             Count of classifications in agreement with human\n",
      "gendermatch                                                  \n",
      "OK                                                        807\n",
      "notOK                                                     193 \n",
      "\n",
      "Analysis of 193 notOK rows\n",
      "             count   %age\n",
      "algo gender              \n",
      "               109   56.5\n",
      "F               29   15.0\n",
      "M               55   28.5\n",
      "All            193  100.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method which loops through the data to compare the algorithm's gender classifications to the human's\n",
    "def label_gendermatch (row):\n",
    "    if row[\"gender_username_algo\"] == row[\"gender_username_human\"]:\n",
    "        return 'OK'\n",
    "    elif (row[\"gender_username_algo\"] == \"\") & (row[\"gender_username_human\"]=='U'):\n",
    "        return 'OK'\n",
    "    else:\n",
    "        return 'notOK'\n",
    "\n",
    "# Populate the gendermatch column with 'OK' if the human and algorithm classifications are the same, else 'not OK' if they are diferent\n",
    "tweets_sample1000_withgender['gendermatch']=tweets_sample1000_withgender.apply (lambda row: label_gendermatch (row), axis=1)\n",
    "\n",
    "# Group the data to aggregate the algorithm's classifications based on count\n",
    "print (tweets_sample1000_withgender.groupby('gender_username_algo').agg({'user': 'size'})\n",
    "      .rename(columns={'user': 'Count of classifications'}), '\\n')\n",
    "\n",
    "# Group the data to aggregate the gendermatch column, based on count\n",
    "print (tweets_sample1000_withgender.groupby(\"gendermatch\").agg({'user': 'size'})\n",
    "      .rename(columns={'user': 'Count of classifications in agreement with human'}), '\\n')\n",
    "\n",
    "# Pivot the data to aggregate the classifications where the gendermatch was not OK\n",
    "print(\"Analysis of 193 notOK rows\")\n",
    "pivot2=pd.pivot_table(tweets_sample1000_withgender[tweets_sample1000_withgender['gendermatch']!='OK'], index=[\"gender_username_algo\"],values=[\"user\"], aggfunc=[len], margins=True)\n",
    "pivot2.columns=['count']\n",
    "pivot2.index.names = ['algo gender']\n",
    "pivot2[\"%age\"] = round(pivot2[\"count\"]/193*100, 1)\n",
    "print(pivot2, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pivot table shows us that the algorithm assigns a gender to (179+259=438) 44% of the 1000 usernames.\n",
    "\n",
    "The second pivot shows that 81% of its decisions (to assign M, F or nothing) are 'OK' (the same as the human) and 19% not the same ('notOK'). \n",
    "\n",
    "The third pivot shows that of the 193 'not OK's, 56.5% are instances where the algorithm made no assignment where the human did. Upon inspection of the data, these were often cases of excluded 3 letter names or words that we prevented the algorithm from matching against to increase the accuracy of the assignments it did make. Abbreviated and unknown names were also a factor here. \n",
    "\n",
    "The other 84 instances (43%) where the algorithm made a prediction of M or F that conflicted with the human's prediction of M/F/U were of importance to us. We examined them further by exporting the data, evaluating both the reason for the algorithm's error and the severity of its error, and re-importing the evaluated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export the dataset for evaluation\n",
    "writer = pd.ExcelWriter('tweets_sample1000_withgender_username.xlsx')\n",
    "tweets_sample1000_withgender.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Problem\n",
      "count                          \n",
      "46                   wordinword\n",
      "19                      surname\n",
      "14               otheruseofname\n",
      "5      unknownname & wordinword \n",
      "\n",
      "                  count\n",
      "Severity               \n",
      "gender incorrect     15\n",
      "gender unknown       69\n",
      "All                  84 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reimport the evaluated dataset\n",
    "probtweets = pd.read_excel(\"84probtweets.xlsx\")\n",
    "\n",
    "pivot4=pd.pivot_table(probtweets, index=[\"Problem\"],values=[\"index\"], aggfunc=[len])\n",
    "pivot4.columns=['count']\n",
    "pivot4 = pivot4.reset_index().sort_values(['count'], ascending=False).set_index(['count'])\n",
    "print(pivot4, '\\n')\n",
    "\n",
    "pivot5=pd.pivot_table(probtweets, index=[\"Severity\"],values=[\"index\"], aggfunc=[len], margins=True)\n",
    "pivot5.columns=['count']\n",
    "print(pivot5, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pivot shows us that the most common reason for an inaccurate gender assignment was \"word in word\", ie a name from the list matching against the username but to a human eye, only as part of a longer word. For example, username \"cgtnamerica\" was a match for female name Erica and so got assigned as female by the algorithm. Other issues were surnames being matched as first names (eg username \"meg_andrews\" matching for male name Andrew) , other uses of names than self-identification confusing the algorithm (eg  username \"MyManjimmyjack\" matching for male name \"Jimmy\") and names unknown to the algorithm leading to incorrect matches of shorter words (eg username \"miss_scarlet\" was a match for male name Carl, as Scarlet wasn't on the names list)\n",
    "\n",
    "The second pivot shows an assessment of the severity of the inaccurate matches. Fifteen are cases where it is clear that the algorithm has matched the wrong gender (eg F where a human can see the gender is clearly M). Sixty-nine are cases where the algorithm has matched a gender of F or M where the human can make no reasonable gender inference one way or the other from the username."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Description\n",
    "\n",
    "Using the same human-classified sample of 1000 random rows from our dataset of tweets, this program crosschecks tokenised words from the description field against a list of keywords known to be reflective of gender, and makes a gender classification where it finds a match.\n",
    "\n",
    "#### Keyword List\n",
    "When inferring gender from the description field (which is essentially the user's self-written profile) a number of oft-repeated key words allowed the human to classify the gender of the writer (eg \"wife\", \"mom\", \"father\").We made a list of 35 such keywords that the human had used when identifying gender from the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>businesswoman</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>businessman</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>congressman</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sisterhood</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fisherman</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gentleman</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>godmother</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>daughter</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>feminism</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>feminist</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>granddad</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>princess</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sorority</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>waitress</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>womanist</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cowgirl</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fangirl</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>goddess</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>husband</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lesbian</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>playboy</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>father</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>female</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>latina</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mother</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tomboy</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sister</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bloke</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>daddy</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mommy</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>queen</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>uncle</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>woman</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>women</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>king</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>aunt</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>dude</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>girl</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>lady</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>mama</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>wife</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>boy</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>dad</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>gal</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>guy</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>mom</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>mum</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>son</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>man</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word gender\n",
       "0   businesswoman      F\n",
       "1     businessman      M\n",
       "2     congressman      M\n",
       "3      sisterhood      F\n",
       "4       fisherman      M\n",
       "5       gentleman      M\n",
       "6       godmother      F\n",
       "7        daughter      F\n",
       "8        feminism      F\n",
       "9        feminist      F\n",
       "10       granddad      M\n",
       "11       princess      F\n",
       "12       sorority      F\n",
       "13       waitress      F\n",
       "14       womanist      F\n",
       "15        cowgirl      F\n",
       "16        fangirl      F\n",
       "17        goddess      F\n",
       "18        husband      M\n",
       "19        lesbian      F\n",
       "20        playboy      M\n",
       "21         father      M\n",
       "22         female      F\n",
       "23         latina      F\n",
       "24         mother      F\n",
       "25         tomboy      F\n",
       "26         sister      F\n",
       "27          bloke      M\n",
       "28          daddy      M\n",
       "29          mommy      F\n",
       "30          queen      F\n",
       "31          uncle      M\n",
       "32          woman      F\n",
       "33          women      F\n",
       "34           king      M\n",
       "35           aunt      F\n",
       "36           dude      M\n",
       "37           girl      F\n",
       "38           lady      F\n",
       "39           mama      F\n",
       "40           wife      F\n",
       "41            boy      M\n",
       "42            dad      M\n",
       "43            gal      F\n",
       "44            guy      M\n",
       "45            mom      F\n",
       "46            mum      F\n",
       "47            son      M\n",
       "48            man      M"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import keyword list\n",
    "descr_keywords = pd.read_excel(\"descr_keywords.xlsx\")\n",
    "descr_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add column to main dataset for new gender prediction based on description\n",
    "# Create a column with username converted to lowercase, on both datasets\n",
    "tweets_sample1000_withgender[\"gender_description_algo\"] = \"\"\n",
    "\n",
    "# Make description field lowercase\n",
    "tweets_sample1000_withgender[\"description\"] = tweets_sample1000_withgender[\"description\"].str.lower()\n",
    "\n",
    "# Import NLTK to tokenize the description field\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tweets_sample1000_withgender[\"description_tokenized\"] = \"\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Run the tokenizing algorithm\n",
    "for i in range(len(tweets_sample1000_withgender)):\n",
    "    if pd.notnull(tweets_sample1000_withgender.at[i, 'description'])==False :\n",
    "        (tweets_sample1000_withgender.at[i, 'description_tokenized'])=\"\"\n",
    "    else:\n",
    "        (tweets_sample1000_withgender.at[i, 'description_tokenized'])=tokenizer.tokenize((tweets_sample1000_withgender.at[i, 'description']))\n",
    "\n",
    "descr_keywords.columns = ['word', 'gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the  gender-assigning algorithm\n",
    "# External loop iterates through the dataset of tweets\n",
    "for i in range(len(tweets_sample1000_withgender)):\n",
    "    # Internal loop iterates through the list of keywords\n",
    "    for j in range(len(descr_keywords)):\n",
    "        # Where a keyword matches to a tokenised word in the description, the corresponding gender is assigned. Lengthier keywords are favoured.\n",
    "        if (descr_keywords.at[j, 'word'] in tweets_sample1000_withgender.at[i, 'description_tokenized']) & (tweets_sample1000_withgender.at[i, 'gender_description_algo']==\"\"):\n",
    "            tweets_sample1000_withgender.at[i, 'gender_description_algo'] = descr_keywords.at[j, 'gender']\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the algorithm's description-based classifications\n",
    "\n",
    "We analysed the algorithm's description-based classifications in terms of quantity and accuracy against the human's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         count\n",
      "gender_description_algo       \n",
      "                           821\n",
      "F                          107\n",
      "M                           72\n",
      "All                       1000 \n",
      "\n",
      "                  count\n",
      "gendermatchdescr       \n",
      "OK                  973\n",
      "notOK                27\n",
      "All                1000 \n",
      "\n",
      "             count  %age\n",
      "algo gender             \n",
      "                 8  29.6\n",
      "F               10  37.0\n",
      "M                9  33.3 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def label_gendermatchdescr (row):\n",
    "    if row[\"gender_description_algo\"] == row[\"gender_description_human\"]:\n",
    "        return 'OK'\n",
    "    elif (row[\"gender_description_algo\"] == \"\") & (row[\"gender_description_human\"]=='U'):\n",
    "        return 'OK'\n",
    "    else:\n",
    "        return 'notOK'\n",
    "\n",
    "tweets_sample1000_withgender['gendermatchdescr']=tweets_sample1000_withgender.apply (lambda row: label_gendermatchdescr (row), axis=1)\n",
    "\n",
    "pivot7=pd.pivot_table(tweets_sample1000_withgender, index=[\"gender_description_algo\"],values=[\"user\"], aggfunc=[len], margins=True)\n",
    "pivot7.columns=['count']\n",
    "print(pivot7, '\\n')\n",
    "\n",
    "pivot8=pd.pivot_table(tweets_sample1000_withgender, index=[\"gendermatchdescr\"],values=[\"user\"], aggfunc=[len], margins=True)\n",
    "pivot8.columns=['count']\n",
    "print(pivot8, '\\n')\n",
    "\n",
    "pivot9=pd.pivot_table(tweets_sample1000_withgender[tweets_sample1000_withgender['gendermatchdescr']!='OK'], index=[\"gender_description_algo\"],values=[\"user\"], aggfunc=[len])\n",
    "pivot9.columns=['count']\n",
    "pivot9.index.names = ['algo gender']\n",
    "pivot9[\"%age\"] = round(pivot9[\"count\"]/27*100, 1)\n",
    "print(pivot9, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The first pivot shows us that the algorithm makes (107F + 72M) 179 gender assignments. \n",
    "\n",
    "The second pivot shows us that the algorithm's gender assigning decisions are 97% OK (the same as the human) and 3% not OK (different to the human).\n",
    "\n",
    "The third pivot shows us that of the 27 'notOK' decisions the algorithm made, 30% were where it assigned no gender when the human did, and 70% were where it assigned an M/F gender that was in conflict with the human's gender assignment based on description.\n",
    "\n",
    "We look at the 27 'not OK' decisions , by exporting the data, marking it up with reasons for the conflict, and re-importing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              reason\n",
      "count               \n",
      "19          otheruse\n",
      "2        his/her/him\n",
      "2          otherword\n",
      "2      pluralisation\n",
      "2          runonword \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Export data\n",
    "writer = pd.ExcelWriter('tweets_sample1000_withgender_description.xlsx')\n",
    "tweets_sample1000_withgender.to_excel(writer,'Sheet1')\n",
    "writer.save()\n",
    "\n",
    "# Import evaluated data, now marked with reasons for the misclassification\n",
    "descrprobtweets = pd.read_excel(\"27probtweets.xlsx\")\n",
    "\n",
    "# Pivot the data to aggregate the reasons for misclassification\n",
    "pivot10=pd.pivot_table(descrprobtweets, index=[\"reason\"],values=[\"description_tokenized\"], aggfunc=[len])\n",
    "pivot10.columns=['count']\n",
    "pivot10 = pivot10.reset_index().sort_values(['count'], ascending=False).set_index(['count'])\n",
    "print(pivot10, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of these incorrect matches are due to 'otheruse': gender-significant words being used in the description in such a way that the human, but not the algorithm, recognised they were not a marker of the user's own gender, eg \"I love my mom\" as opposed to \"I'm a mom\". Other issues that caused problems were \"his/her/him\" (a description such as \"just an introvert forever trying to break out of her shell\" allowed the human to identify the writer was female, but not the algorithm as these gender pronouns were not in its list of keywords), \"otherword\" (a keyword unknknown to the algorithm eg \"damsel\", \"gentlemen\"), \"pluralisation\" (pluraised keywords in the description eg \"sisters\", \"aunts\" were not matched) and run on words (a side effect of the tokenised description is that when words run into another in the description, eg \"latinafeminist\" \"bigmomma\" they do not return a match for the gender-significant words.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Combining Username & Description\n",
    "\n",
    "The final program combines the username and description programs to classify a maximum of rows with a gender. In the 1.5% of cases where the two subprograms produced a conflicting M/F classification, the description program's classification takes precedence, as it had been found to be more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              len\n",
      "                                             user\n",
      "gender_username_algo gender_description_algo     \n",
      "                                              466\n",
      "                     F                         56\n",
      "                     M                         40\n",
      "F                                             133\n",
      "                     F                         41\n",
      "                     M                          5\n",
      "M                                             222\n",
      "                     F                         10\n",
      "                     M                         27\n"
     ]
    }
   ],
   "source": [
    "# Initial aggregated comparison of the classifications made by the username and description programs\n",
    "print(pd.pivot_table(tweets_sample1000_withgender, index=[\"gender_username_algo\", \"gender_description_algo\"],values=[\"user\"], aggfunc=[len]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The pivot above shows the 5 cases where the username-based program classified F and the description-based program classified M, plus the 10 cases where the username-based program classified M and the description-based program classified F, for a total of 15 conflicting classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new column for the final gender classification based on the two programs\n",
    "tweets_sample1000_withgender[\"gender_algo_final\"] = \"\"\n",
    "\n",
    "for i in range(len(tweets_sample1000_withgender)):\n",
    "    # Where the description program has made a classification, put that in the final classification column\n",
    "    tweets_sample1000_withgender.at[i, \"gender_algo_final\"] = tweets_sample1000_withgender.at[i, \"gender_description_algo\"] \n",
    "    # If the description program didn't make a classification, use the username program's classification\n",
    "    if tweets_sample1000_withgender.at[i, \"gender_algo_final\"]==\"\":\n",
    "        tweets_sample1000_withgender.at[i, \"gender_algo_final\"]=tweets_sample1000_withgender.at[i, \"gender_username_algo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the final classifications made\n",
    "\n",
    "We analyse the final classifications as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   len\n",
      "                  user\n",
      "gender_algo_final     \n",
      "                   466\n",
      "F                  240\n",
      "M                  294\n",
      "                  len\n",
      "                 user\n",
      "gendermatchfinal     \n",
      "OK                825\n",
      "notOK             175\n"
     ]
    }
   ],
   "source": [
    "# Pivot to view the number of classifications made\n",
    "print(pd.pivot_table(tweets_sample1000_withgender, index=[\"gender_algo_final\"],values=[\"user\"], aggfunc=[len]))\n",
    "\n",
    "# Method to compare the human's final gender classification against the algorithmical final classification\n",
    "def label_gendermatch_overall (row):\n",
    "    if row[\"gender_algo_final\"] == row[\"gender_human_final\"]:\n",
    "        return 'OK'\n",
    "    elif (row[\"gender_algo_final\"] == \"\") & (row[\"gender_human_final\"]=='U'):\n",
    "        return 'OK'\n",
    "    else:\n",
    "        return 'notOK'\n",
    "\n",
    "# Apply the above method to each row to mark the classifications as correct or not\n",
    "tweets_sample1000_withgender[\"gendermatchfinal\"] = \"\"    \n",
    "tweets_sample1000_withgender['gendermatchfinal']=tweets_sample1000_withgender.apply (lambda row: label_gendermatch_overall (row), axis=1)\n",
    "\n",
    "# Pivot to view the accuracy of classificatoins made\n",
    "print(pd.pivot_table(tweets_sample1000_withgender, index=[\"gendermatchfinal\"],values=[\"user\"], aggfunc=[len]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pivot table shows us that the algorithm can assign a gender to 53.4% of the dataset (240F + 294M = 534) using a combination of gender and username.\n",
    "\n",
    "The second table shows us the final gender assignment produced by the algorithm (the combination of username and description) compared to the human's final gender assignment. We have an overall accuracy rate of 82.5%, which is sufficient for our purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final rule-based program, run over the dataset in Appendix 7, is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PREPARATION OF NAME DATA\n",
    "\n",
    "# Import dataset of names and their genders\n",
    "namesfinal = pd.read_excel(\"top2000names2014final.xlsx\")\n",
    "\n",
    "# Sort names dataframe so longest names come first (so that Erica will then match against Erica and not Eric)\n",
    "namesfinal[\"namelength\"] = \"\"\n",
    "for i in range(len(namesfinal)):\n",
    "    namesfinal.at[i, \"namelength\"] = len(namesfinal.at[i, \"name\"])\n",
    "namesfinal = namesfinal.sort_values(by=['namelength'], ascending=False)\n",
    "namesfinal = namesfinal.reset_index(drop=True)\n",
    "print(namesfinal.shape)\n",
    "\n",
    "# Remove any names 3 characters or less\n",
    "namesfinal.drop(namesfinal[namesfinal.namelength < 4].index, inplace=True)\n",
    "print(namesfinal.shape)\n",
    "\n",
    "# Create a column with name converted to lowercase\n",
    "namesfinal[\"name_lower\"] = namesfinal[\"name\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ASSIGNMENT OF GENDER BASED ON USERNAME\n",
    "\n",
    "# Create a column with username converted to lowercase\n",
    "OURDATASET[\"user_lower\"] = OURDATASET[\"user\"].str.lower()\n",
    "\n",
    "# Create 2 columns for the algorithm to make its gender prediction in\n",
    "OURDATASET[\"gender_username_algo\"] = \"\"\n",
    "OURDATASET[\"namematched_algo\"] = \"\"\n",
    "\n",
    "# Run the name matching algorithm\n",
    "for i in range(len(OURDATASET)):\n",
    "    for j in range (len(namesfinal)):\n",
    "        if OURDATASET.at[i, \"gender_username_algo\"]==\"\":\n",
    "            if (namesfinal.at[j, \"name_lower\"] in OURDATASET.at[i, \"user_lower\"]):\n",
    "                OURDATASET.at[i, \"gender_username_algo\"] = namesfinal.at[j, \"gender\"]\n",
    "                OURDATASET.at[i, \"namematched_algo\"] = namesfinal.at[j, \"name\"]\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "    # Just to get a sense of progress while processing\n",
    "    if i%300==0:\n",
    "        (print(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ASSIGNMENT OF GENDER BASED ON DESCRIPTION\n",
    "\n",
    "descr_keywords = pd.read_excel(\"descr_keywords.xlsx\")\n",
    "\n",
    "# Add column for new gender prediction based on description\n",
    "# Create a column with username converted to lowercase, on both datasets\n",
    "OURDATASET[\"gender_description_algo\"] = \"\"\n",
    "\n",
    "# Make description field lowercase\n",
    "OURDATASET[\"description\"] = OURDATASET[\"description\"].str.lower()\n",
    "\n",
    "# Tokenize the description field using natural language toolkit\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "OURDATASET[\"description_tokenized\"] = \"\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Run the tokenizing algorithm\n",
    "for i in range(len(OURDATASET)):\n",
    "    if pd.notnull(OURDATASET.at[i, 'description'])==False :\n",
    "        (OURDATASET.at[i, 'description_tokenized'])=\"\"\n",
    "    else:\n",
    "        (OURDATASET.at[i, 'description_tokenized'])=tokenizer.tokenize((OURDATASET.at[i, 'description']))\n",
    "\n",
    "descr_keywords.columns = ['word', 'gender']\n",
    "\n",
    "# Run the gender assigning algorithm\n",
    "for i in range(len(OURDATASET)):\n",
    "    for j in range(len(descr_keywords)):\n",
    "        if (descr_keywords.at[j, 'word'] in OURDATASET.at[i, 'description_tokenized']) & (OURDATASET.at[i, 'gender_description_algo']==\"\"):\n",
    "            OURDATASET.at[i, 'gender_description_algo'] = descr_keywords.at[j, 'gender']\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ASSIGNMENT OF FINAL GENDER VALUE\n",
    "\n",
    "OURDATASET[\"gender_final\"] = \"\"\n",
    "\n",
    "for i in range(len(OURDATASET)):\n",
    "    OURDATASET.at[i, \"gender_final\"] = OURDATASET.at[i, \"gender_description_algo\"] \n",
    "    if OURDATASET.at[i, \"gender_final\"]==\"\":\n",
    "        OURDATASET.at[i, \"gender_final\"]=OURDATASET.at[i, \"gender_username_algo\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
